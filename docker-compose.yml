services:
  # 模型下載服務（第一次啟動會下載模型檔案，完成後容器保持停止狀態）
  model-downloader:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ai-face-swap-model-downloader
    volumes:
      - ./backend:/app
    command: >
      sh -c "
        set -e
        MODEL_PATH=/app/models/inswapper_128.onnx
        mkdir -p /app/models

        if [ -f \"$$MODEL_PATH\" ]; then
          echo '✅ AI 模型已存在，跳過下載'
          FILE_SIZE=$$(stat -c%s \"$$MODEL_PATH\" 2>/dev/null || echo \"0\")
          FILE_SIZE=$${FILE_SIZE:-0}
          echo \"📊 現有檔案大小: $$FILE_SIZE bytes\"
          ls -lh \"$$MODEL_PATH\"
          exit 0
        fi

        echo '🔄 正在下載 AI 模型檔案...'

        download_from() {
          URL=$$1
          SOURCE_LABEL=$$2
          echo \"📥 嘗試從 $$SOURCE_LABEL 下載...\"
          if wget --timeout=60 --tries=3 --progress=bar:force:noscroll -O \"$$MODEL_PATH\" \"$$URL\"; then
            return 0
          fi
          return 1
        }

        download_from 'https://huggingface.co/spaces/mkrzyzan/face-swap/resolve/main/inswapper_128.onnx' 'HuggingFace' || \
        download_from 'https://github.com/facefusion/facefusion-assets/releases/download/models/inswapper_128.onnx' 'GitHub'

        if [ ! -f \"$$MODEL_PATH\" ]; then
          echo '❌ 模型檔案下載失敗'
          exit 1
        fi

        FILE_SIZE=$$(stat -c%s \"$$MODEL_PATH\" 2>/dev/null || echo \"0\")
        FILE_SIZE=$${FILE_SIZE:-0}
        echo \"📊 下載檔案大小: $$FILE_SIZE bytes\"

        if [ $$FILE_SIZE -lt 200000000 ]; then
          echo '❌ 檔案太小，可能下載不完整'
          rm -f \"$$MODEL_PATH\"
          exit 1
        fi

        if python3 -c \"import onnx; onnx.load('$$MODEL_PATH')\" 2>/dev/null; then
          echo '✅ 模型檔案驗證完成'
        else
          echo '⚠️ 無法驗證 ONNX 格式，但檔案大小正常'
        fi

        ls -lh \"$$MODEL_PATH\"
      "
    restart: "no"

  frontend:
    image: docker.io/library/nginx:alpine
    container_name: ai-face-swap-frontend
    ports:
      - "8882:80"
    volumes:
      - ./frontend:/usr/share/nginx/html
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./backend/results:/results:ro
      - ./backend/uploads:/uploads:ro
    depends_on:
      - backend
    restart: unless-stopped

  backend:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ai-face-swap-backend
    expose:
      - "3001"
    volumes:
      # 一次全掛載整個後端目錄
      - ./backend:/app
    depends_on:
      - model-downloader
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G
    environment:
      - PYTHONPATH=/app
      - ENVIRONMENT=development
    privileged: true
    # 啟動 uvicorn，調整 backlog 與 keep-alive 以提升高併發連線能力
    command: [
      "python", "-m", "uvicorn", "app:app",
      "--host", "0.0.0.0",
      "--port", "3001",
      "--workers", "1",
      "--backlog", "65535",
      "--timeout-keep-alive", "600",
      "--limit-max-requests", "100000",
      "--log-level", "warning"
    ]

networks:
  default:
    name: ai-avatar-studio-network
